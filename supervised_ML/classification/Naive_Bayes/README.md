# Наивный байесовский классификатор (Naive Bayes classifier)

> Наивный Байес – это самый простой алгоритм, который вы можете применить к своим данным. 
Как следует из названия, этот алгоритм делает наивное предположение, что все переменные в наборе данных не коррелируют друг с другом. 
Это действительно очень наивно думать что данные об объектах не связанны между собой. 
Удивительно, но этот подход на удивление хорошо работает даже с данными, связанными между собой.

## Краткое введение в теорему Байеса

В общем виде теорема Байеса описывается формулой 
выбора наилучшей **гипотезы** на основе **данных**.

Когда мы применяем теорему Байеса в контексе машинного обучения в задаче классификации, - 
нашей гипотезой выступает класс (class) который должен быть присвоен исходя из данных (data).

Теорема Байеса имеет вид:

**P**(*class*|*data*) = (**P**(*data*|*class*) * **P**(*class*)) / **P**(*data*)

где

 - **P**(*class*|*data*) вероятность классификации с учетом данных. Это называется **апостериорной вероятностью**.
 - **P**(*data*|*class*) вероятность данных, учитывая, что класс определен верно.
 - **P**(*class*) вероятность того, что класс определен верно (независимо от данных). Это называется **априорной вероятностью**.
 - **P**(*data*) вероятность данных (независимо от класса).

## Типы наивных байесовских классификаторов

Существует три типа наивных байесовских классификаторов, которые зависят 
от распределения набора данных, а именно: 

 - **Гауссовский**: используется, когда набор данных распределен нормально. 
 - **Полиномиальный**: используется, когда набор данных содержит дискретные значения. 
 - **Бернулли**: Используется при работе над задачами бинарной классификации.

## Плюсы и минусы наивного байесовского алгоритма

### Плюсы

 - Алгоритм легко и быстро предсказывает класс тестового набора данных. Он также хорошо 
   справляется с многоклассовым прогнозированием.
 - Производительность наивного байесовского классификатора лучше, чем у других простых 
   алгоритмов, таких как логистическая регрессия. Более того, вам требуется меньше обучающих 
   данных. 
 - Он хорошо работает с категориальными признаками(по сравнению с числовыми). 
   Для числовых признаков предполагается нормальное распределение, что может быть серьезным 
   допущением в точности нашего алгоритма. 

### Минусы  

 - Если переменная имеет категорию (в тестовом наборе данных), которая не наблюдалась в обучающем наборе данных, то модель присвоит 0 (нулевую) вероятность и не сможет сделать предсказание. Это часто называют нулевой частотой. Чтобы решить эту проблему, мы можем использовать технику сглаживания. Один из самых простых методов сглаживания называется оценкой Лапласа.
 - Значения спрогнозированных вероятностей, возвращенные методом predict_proba, не всегда являются достаточно точными.
 - Ограничением данного алгоритма является предположение о независимости признаков. Однако в реальных задачах полностью независимые признаки встречаются крайне редко.

### Для каких задач можно использовать наивного Байеса?

Таким образом, идеальные области для применения наивного байесовского классификатора это:

 - Система рекомендаций. В сочетании алгоритма с методами коллаборативной фильтрации (Collaborative Filtering) 
   мы можем создать рекомендательную систему, которая использует машинное обучение 
   и методы добычи данных для учета невидимой информации (такой, как поиск фильмов 
   пользователем и длительность просмотра). Цель – предсказание того, понравится ли 
   пользователю данный ресурс/продукт или нет.
 - Фильтрация спама и классификация текста. Наивный байесовский классификатор в основном 
   используются для классификации текстов (благодаря лучшему результату в многоклассовых 
   проблемах и правилу независимости) и имеет более высокую точность по сравнению с другими 
   алгоритмами. В результате он широко используется в фильтрации спама (в электронной почте) 
   и анализе настроений (к примеру, социальных сетей, для выявления положительных 
   и отрицательных настроений клиентов).